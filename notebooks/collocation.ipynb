{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "lang101",
   "display_name": "lang101",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "String processing with Python\n",
    "\n",
    "Using a text corpus found on the cds-language GitHub repo or a corpus of your own found on a site such as Kaggle, write a Python script which calculates collocates for a specific keyword.\n",
    "\n",
    "\n",
    "\n",
    "The script should take a directory of text files, a keyword, and a window size (number of words) as input parameters, and an output file called out/{filename}.csv\n",
    "These parameters can be defined in the script itself\n",
    "Find out how often each word collocates with the target across the corpus\n",
    "Use this to calculate mutual information between the target word and all collocates across the corpus\n",
    "Save result as a single file consisting of four columns: collocate, raw_frequency, MI\n",
    "\n",
    "\n",
    "BONUS CHALLENGE: Use argparse to take inputs from the command line as parameters\n",
    "\n",
    "\n",
    "General instructions\n",
    "\n",
    "For this assignment, you should upload a standalone .py script which can be executed from the command line.\n",
    "Save your script as collocation.py\n",
    "Make sure to include a requirements.txt file and your data\n",
    "You can either upload the scripts here or push to GitHub and include a link - or both!\n",
    "Your code should be clearly documented in a way that allows others to easily follow the structure of your script and to use them from the command line\n",
    "\n",
    "\n",
    "Purpose\n",
    "\n",
    "This assignment is designed to test that you have a understanding of:\n",
    "\n",
    "how to structure, document, and share a Python scripts;\n",
    "how to effectively make use of native Python packages for string processing;\n",
    "how to extract basic linguistic information from large quantities of text, specifically in relation to a specific target keyword"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary modules\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import *\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "filepath = os.path.join(\"..\", \"data\", \"100_english_novels\", \"corpus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function which includes the arguments text directory, keyword and window size (the latter n-words before and n-words after keyword)\n",
    "def collocation(text_dir, keyword, window_size = 1):\n",
    "    # Make a list that the loop appends to\n",
    "    collocations = list()\n",
    "    collocations_unique = list()\n",
    "    concordance_lines = list()\n",
    "    collocate_lines = list()\n",
    "    n_collocate_occurences = list()\n",
    "    N = 0\n",
    "\n",
    "    # For each file in the filepath that ends with .txt, read the file into \"text\"\n",
    "    for file in Path(text_dir).glob(\"*9.txt\"):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "            # Tokenize each text into individual words\n",
    "            text_tokens = re.compile(r\"\\W+\").split(text)\n",
    "            N += len(text_tokens)\n",
    "            \n",
    "            # Return index for each element text_tokens if the element in text_tokens is equal to keyword\n",
    "            indices = [index for index, match in enumerate(text_tokens) if match == keyword]\n",
    "            \n",
    "            # For each keyword in the text, create an object (= concordance_line) that has keyword and the words just before and after (keyword +- window_size)\n",
    "            for index in indices:\n",
    "                concordance_line = text_tokens[max(0,index - window_size):index+window_size+1]\n",
    "                \n",
    "                # Append the concordance line to \"concordance_lines\"\n",
    "                concordance_lines.append(concordance_line)\n",
    "\n",
    "                # For each word in the concordance_line, add it to \"new_collocations\" if it is not the keyword.\n",
    "                new_collocations = [collocate for collocate in concordance_line if collocate != keyword]\n",
    "\n",
    "                # For each word in the concordance_line, add it to \"new_collocations_unique\" if it is not the keyword and if it does not already exist in the list.\n",
    "                new_collocations_unique = [collocate for collocate in concordance_line if collocate not in collocations_unique and collocate != keyword]\n",
    "                \n",
    "                # Extend my list collocations, with all the collocations (words around keyword)\n",
    "                collocations.extend(new_collocations)\n",
    "                \n",
    "                # Extend my list collocations_unique, with all the collocations (words around keyword) that do not already appear in the list.\n",
    "                if new_collocations_unique not in collocations_unique:\n",
    "                    collocations_unique.extend(new_collocations_unique)\n",
    "    \n",
    "    # o21 # n-times collocate occurs w/o keyword (same as: n(collocate) minus o11)\n",
    "    for file in Path(text_dir).glob(\"*9.txt\"):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "            # Tokenize each text into individual words\n",
    "            text_tokens = re.compile(r\"\\W+\").split(text)\n",
    "\n",
    "            #\n",
    "            for unique_collocate in collocations_unique:\n",
    "                indices = [i for i, x in enumerate(text_tokens) if x == unique_collocate]\n",
    "                n_collocate_occur = {unique_collocate : len(indices)}\n",
    "                n_collocate_occurences.append(n_collocate_occur)\n",
    "    \n",
    "    # Define dictionary to loop into\n",
    "    n_collocate_dict = Counter({})\n",
    "\n",
    "    #\n",
    "    for dic in n_collocate_occurences:\n",
    "        # If dictionary is empty\n",
    "        if bool(n_collocate_dict) == False:\n",
    "            n_collocate_dict = Counter(dic)\n",
    "        dic = Counter(dic)\n",
    "        n_collocate_dict = n_collocate_dict + dic\n",
    "    \n",
    "    # Go through the collocations (all words that have appeared with the keyword in any of the texts) and count how often they have occured with the keyword.\n",
    "    o11 = Counter(collocations)\n",
    "\n",
    "    # o21 # n-times collocate occurs w/o keyword (same as: n(collocate) minus o11)\n",
    "    o21 = n_collocate_dict - o11\n",
    "\n",
    "    # Create an empty dictionary for counting concordance lines where keyword occurs without collocation.\n",
    "    o12 = dict()\n",
    "    \n",
    "    # For each unique collocation in collocations_unique:\n",
    "    for collocation_unique in collocations_unique:\n",
    "        \n",
    "        # Set loop counter\n",
    "        loop_count = 0\n",
    "        \n",
    "        # For each line in concordance_lines, if the unique collocation does NOT appear, add +1 to counter\n",
    "        for concordance_line in concordance_lines:\n",
    "            if collocation_unique not in concordance_line:\n",
    "                loop_count += 1\n",
    "\n",
    "        # Updating the o12 to include a count for n-times that each unique collocation did not appear with a keyword\n",
    "        o12.update({collocation_unique : loop_count})\n",
    "    \n",
    "    # Getting number of concordance lines (and we have 1 per keyword)\n",
    "    n_times_keyword = len(concordance_lines)\n",
    "    \n",
    "    # Writing a dictionary with the keys for all collocates, and the value for n_keywords.\n",
    "    R1 = Counter({x: n_times_keyword for x in o12})\n",
    "\n",
    "    # Calculating C1:\n",
    "    C1 = o11 + o21 # C1 # n-times collocate occurs (regardless of keyword) (same as o11 + o21)\n",
    "    \n",
    "    # Calculating E11:\n",
    "    # We know the formula: (R1*C1)/N, \n",
    "    # #Therefore we first calculate R1*C1:\n",
    "    R1_multiplied_with_C1 = {k: R1[k]*C1[k] for k in R1} # Dictionary comprehension for multiplying values between dictionaries for the same keys\n",
    "    # Then we multiply each of the key-value pairs with N. This gets us E11\n",
    "    E11 = Counter({k:v/N for (k,v) in R1_multiplied_with_C1.items()})\n",
    "\n",
    "    # Calculating MI\n",
    "    # We know the formula: log(o11/e11)\n",
    "    # First we then divide o11 with e11\n",
    "    o11_divided_by_E11 = {k: o11[k] / float(E11[k]) for k in E11 if k in o11}\n",
    "    # Then we take the log of each to get MI-scores\n",
    "    MI = {k:np.log(v) for (k,v) in o11_divided_by_E11.items()}\n",
    "\n",
    "    # Create a pandas to write\n",
    "    df_mi = pd.DataFrame.from_dict(MI, orient=\"index\",\n",
    "                    columns=[\"mi\"])\n",
    "    df_raw_freq = pd.DataFrame.from_dict(o11, orient=\"index\",\n",
    "                    columns=[\"raw_freq\"])\n",
    "    \n",
    "    df = df_mi.join(df_raw_freq)\n",
    "    df['collocate'] = df.index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = df[['collocate', 'raw_freq', 'mi']]\n",
    "    \n",
    "    # Create path for output file\n",
    "    outpath = os.path.join(\".\", f\"{keyword}_collocations_info.csv\")\n",
    "    df.to_csv(outpath)\n",
    "    \n",
    "    print(f\"A new file has been created: {outpath}\")\n",
    "\n",
    "    #print(f\"N-times collocates occurs in the text (regardless of keyword): {n_collocate_dict}\")\n",
    "    #print(f\"All concordance lines with keyword: {concordance_lines}\")\n",
    "    #print(f\"All collocations (not unique): {collocations}\")\n",
    "    #print(f\"All unique collocations: {collocations_unique}\")\n",
    "    #print(f\"Number of collocations: {len(collocations)}\")\n",
    "    #print(f\"Number of unique collocations: {len(collocations_unique)}\")\n",
    "    #print(f\"Number of tokens, total: {N}\") # # N # Number of words (or sum of C1 + C2 or sum of R1 + R2)\n",
    "    #print(f\"Raw frequency - N-times keyword occurs with collocate: {o11}\") # Raw-frequency! (n-times keyword occurs with collocate)\n",
    "    #print(f\"N-times keyword occurs w/o collocate: {o12}\") # n-times keyword occurs w/o collocate\n",
    "    #print(f\"N-times keyword occurs (regardless of collocate): {R1}\") # n-times keyword occurs (regardless of collocate)\n",
    "    #print(f\"N-times collocates occurs w/o keyword: {o21}\") # o21 # n-times collocate occurs w/o keyword (same as: n(collocate) minus o11)\n",
    "    #print(f\"N-times collocates occurs (regardless of keyword): {C1}\") # C1 # n-times collocate occurs (regardless of keyword) (same as o11 + o21)\n",
    "    #print(f\"E11 (Expected frequency, given the total frequency of the keyword + collocates): {E11}\")\n",
    "    #print(f\"MI): {MI}\") # MI's for all collocates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A new file has been created: .\\denounce_collocations_info.csv\n"
     ]
    }
   ],
   "source": [
    "collocation(filepath, \"denounce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}